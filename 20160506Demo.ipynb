{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那酸\n",
      "酸民\n",
      "11\n",
      "那酸\n",
      "酸民\n",
      "民婉\n",
      "婉君\n",
      "君也\n",
      "也可\n",
      "可以\n",
      "以報\n",
      "報名\n",
      "名嗎\n"
     ]
    }
   ],
   "source": [
    "input_sentence = '那酸民婉君也可以報名嗎'\n",
    "print input_sentence.decode('utf-8')[0:2]\n",
    "print input_sentence.decode('utf-8')[1:3]\n",
    "print len(input_sentence.decode('utf-8'))\n",
    "#for \n",
    "\n",
    "sentence = input_sentence.decode('utf-8')\n",
    "for i in range(0, len(sentence) -1 ):\n",
    "    print sentence[i:i+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tri-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那酸民\n",
      "酸民婉\n",
      "11\n",
      "那酸民\n",
      "酸民婉\n",
      "民婉君\n",
      "婉君也\n",
      "君也可\n",
      "也可以\n",
      "可以報\n",
      "以報名\n",
      "報名嗎\n"
     ]
    }
   ],
   "source": [
    "input_sentence = '那酸民婉君也可以報名嗎'\n",
    "print input_sentence.decode('utf-8')[0:3]\n",
    "print input_sentence.decode('utf-8')[1:4]\n",
    "print len(input_sentence.decode('utf-8'))\n",
    "#for \n",
    "\n",
    "sentence = input_sentence.decode('utf-8')\n",
    "for i in range(0, len(sentence) -2 ):\n",
    "    print sentence[i:i+3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那酸\n",
      "酸民\n",
      "民婉\n",
      "婉君\n",
      "君也\n",
      "也可\n",
      "可以\n",
      "以報\n",
      "報名\n",
      "名嗎\n",
      "那酸民\n",
      "酸民婉\n",
      "民婉君\n",
      "婉君也\n",
      "君也可\n",
      "也可以\n",
      "可以報\n",
      "以報名\n",
      "報名嗎\n"
     ]
    }
   ],
   "source": [
    "input_sentence = '那酸民婉君也可以報名嗎'\n",
    "\n",
    "\n",
    "def ngram(input_sentence, n = 2):\n",
    "    sentence = input_sentence.decode('utf-8')\n",
    "    for i in range(0, len(sentence) - n + 1 ):\n",
    "        print sentence[i:i+n]\n",
    "        \n",
    "ngram(input_sentence, 2)\n",
    "ngram(input_sentence, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'c': 1, 'b': 2}\n",
      "[('a', 2), ('c', 1), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "ary = ['a' , 'b', 'a', 'c', 'b']\n",
    "dic = {}\n",
    "for ele in ary:\n",
    "    if ele not in dic:\n",
    "        dic[ele] = 1\n",
    "    else:\n",
    "        dic[ele] = dic[ele] + 1\n",
    "print dic\n",
    "print dic.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', 1), ('a', 2), ('b', 2)]\n",
      "[('a', 2), ('b', 2), ('c', 1)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "swd = sorted(dic.items(), key = operator.itemgetter(1))\n",
    "print swd\n",
    "\n",
    "swd = sorted(dic.items(), key = operator.itemgetter(1), reverse=True)\n",
    "print swd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counter \n",
    "- https://docs.python.org/2/library/collections.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 2, 'b': 2, 'c': 1})\n",
      "[('a', 2), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter()\n",
    "for ele in ary:\n",
    "    c[ele] += 1\n",
    "print c\n",
    "\n",
    "print c.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(input_sentence, n = 2):\n",
    "    dic = {}\n",
    "    sentence = input_sentence.decode('utf-8')\n",
    "    for i in range(0, len(sentence) - n + 1 ):\n",
    "        segment = sentence[i:i+n]\n",
    "        if segment not in dic:\n",
    "            dic[segment] = 1\n",
    "        else:\n",
    "            dic[segment] = dic[segment] + 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'\\u5a49\\u541b': 1, u'\\u5831\\u540d': 1, u'\\u53ef\\u4ee5': 1, u'\\u540d\\u55ce': 1, u'\\u541b\\u4e5f': 1, u'\\u4e5f\\u53ef': 1, u'\\u90a3\\u9178': 1, u'\\u6c11\\u5a49': 1, u'\\u9178\\u6c11': 1, u'\\u4ee5\\u5831': 1}\n",
      "一樣 1\n",
      "氣是 1\n",
      "是否 1\n",
      "氣一 1\n",
      "否跟 1\n",
      "天天 2\n",
      "昨天 1\n",
      "今天 1\n",
      "跟昨 1\n",
      "天氣 2\n"
     ]
    }
   ],
   "source": [
    "print ngram(input_sentence, 2)\n",
    "dic = ngram(\"今天天氣是否跟昨天天氣一樣\", 2)\n",
    "for ele in dic:\n",
    "    print ele, dic[ele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(input_sentence, n = 2):\n",
    "    c = Counter()\n",
    "    sentence = input_sentence.decode('utf-8')\n",
    "    for i in range(0, len(sentence) - n + 1 ):\n",
    "        segment = sentence[i:i+n]\n",
    "        c[segment] += 1        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({u'\\u4e00\\u6a23': 1,\n",
       "         u'\\u4eca\\u5929': 1,\n",
       "         u'\\u5426\\u8ddf': 1,\n",
       "         u'\\u5929\\u5929': 2,\n",
       "         u'\\u5929\\u6c23': 2,\n",
       "         u'\\u6628\\u5929': 1,\n",
       "         u'\\u662f\\u5426': 1,\n",
       "         u'\\u6c23\\u4e00': 1,\n",
       "         u'\\u6c23\\u662f': 1,\n",
       "         u'\\u8ddf\\u6628': 1})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram(\"今天天氣是否跟昨天天氣一樣\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          1500\n",
       "category    1500\n",
       "view_cnt    1500\n",
       "time        1500\n",
       "title       1500\n",
       "summary     1500\n",
       "link        1500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3 as lite\n",
    "import pandas as pd\n",
    "with lite.connect('news.sqlite') as db:\n",
    "    df = pd.read_sql_query('SELECT * FROM news_entry;', db)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "】\n",
      "【\n",
      "？\n",
      ".\n",
      "、\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "df.ix[1]['title'].encode('utf-8')\n",
    "\n",
    "\n",
    "skip_word = '】【？.、'.decode('utf-8') \n",
    "for w in skip_word:\n",
    "    print w\n",
    "    \n",
    "a = '你好嗎？'\n",
    "print '？' in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = ngram(df.ix[1]['title'].encode('utf-8'), 2)\n",
    "for ele in c:\n",
    "    ary = []\n",
    "    for w in skip_word:\n",
    "        if w in ele: \n",
    "            ary.append(w)\n",
    "    if len(ary) == 0:\n",
    "        print ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人不\n",
      "  \n",
      "不會\n",
      "割喉\n",
      "更新\n",
      "會判\n",
      "喉殺\n",
      "2人\n",
      " 割\n",
      "殺1\n",
      "判死\n"
     ]
    }
   ],
   "source": [
    "c = ngram(df.ix[1]['title'].encode('utf-8'), 2)\n",
    "for ele in c:\n",
    "    invalid = len([w for w in skip_word if w in ele])\n",
    "    if invalid == 0:\n",
    "        print ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip_word = '】【？.、 」　'.decode('utf-8') \n",
    "def ngram2(input_sentence, n = 2):\n",
    "    c = Counter()\n",
    "    sentence = input_sentence.decode('utf-8')\n",
    "    for i in range(0, len(sentence) - n + 1 ):\n",
    "        segment = sentence[i:i+n]\n",
    "        invalid = len([w for w in skip_word if w in segment]) \n",
    "        if invalid ==0:\n",
    "            c[segment] += 1        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人不 1\n",
      "喉殺 1\n",
      "不會 1\n",
      "更新 1\n",
      "會判 1\n",
      "2人 1\n",
      "割喉 1\n",
      "殺1 1\n",
      "判死 1\n"
     ]
    }
   ],
   "source": [
    "title =  df.ix[1]['title'].encode('utf-8')\n",
    "c = ngram2(title, 2)\n",
    "for ele in c:\n",
    "    print ele, c[ele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = df['title']\n",
    "c2 = Counter()\n",
    "for title in titles:\n",
    "    c = ngram2(title.encode('utf-8'), 2)\n",
    "    for ele in c:\n",
    "        c2[ele] += c[ele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "廣R 81\n",
      "更新 80\n",
      "有片 66\n",
      "台灣 45\n",
      "TI 42\n",
      "央廣 41\n",
      "20 41\n",
      "法廣 40\n",
      "RF 40\n",
      "FI 40\n",
      "RT 38\n",
      "10 38\n",
      "中國 34\n",
      "週刊 34\n",
      "壹週 34\n",
      "北市 33\n",
      "影片 29\n",
      "00 26\n",
      "蘋果 22\n",
      "網友 21\n",
      "媽媽 20\n",
      "新聞 19\n",
      "川普 19\n",
      "全球 19\n",
      "美國 18\n",
      "英文 18\n",
      "O雙 16\n",
      "MO 16\n",
      "OM 16\n",
      "母親 16\n"
     ]
    }
   ],
   "source": [
    "dic = c2.most_common(30)\n",
    "for ele in dic:\n",
    "    print ele[0], ele[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = df['title']\n",
    "c2 = Counter()\n",
    "for title in titles:\n",
    "    c = ngram2(title.encode('utf-8'), 3)\n",
    "    for ele in c:\n",
    "        c2[ele] += c[ele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "央廣R 41\n",
      "廣RF 40\n",
      "法廣R 40\n",
      "RFI 40\n",
      "廣RT 38\n",
      "RTI 38\n",
      "壹週刊 34\n",
      "OMO 16\n",
      "O雙語 16\n",
      "MO雙 16\n",
      "TOM 16\n",
      "雙語爆 16\n",
      "王建民 14\n",
      "母親節 14\n",
      "鄭性澤 14\n",
      "蔡英文 13\n",
      "520 10\n",
      "環境資 8\n",
      "國民黨 8\n",
      "大巨蛋 8\n",
      "境資訊 8\n",
      "希拉蕊 8\n",
      "北市府 8\n",
      "巴拿馬 7\n",
      "200 7\n",
      "00萬 6\n",
      "拿馬文 6\n",
      "問蘋果 6\n",
      "溫度計 6\n",
      "201 6\n"
     ]
    }
   ],
   "source": [
    "dic = c2.most_common(30)\n",
    "for ele in dic:\n",
    "    print ele[0], ele[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python One Line Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['APPLE', 'BANANA', 'COKE', 'DRINK']\n",
      "['APPLE', 'BANANA', 'COKE', 'DRINK']\n"
     ]
    }
   ],
   "source": [
    "a = ['apple', 'banana', 'coke', 'Drink']\n",
    "'apple'.upper()\n",
    "\n",
    "ary = []\n",
    "for w in a:\n",
    "    ary.append(w.upper())\n",
    "print ary\n",
    "\n",
    "print [w.upper() for w in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COKE']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['COKE']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['apple', 'banana', 'coke', 'Drink']\n",
    "\n",
    "ary = []\n",
    "for w in a:\n",
    "    if 'c' in w:\n",
    "        ary.append(w.upper())\n",
    "print ary\n",
    "\n",
    "[w.upper() for w in a if 'c' in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新\n",
      "新增影片 華為今在台發表最新旗艦機P9\n",
      "主打拍照功能\n",
      "手機搭載兩顆徠卡\n",
      "Leica\n",
      "光學鏡頭\n",
      "讓照片更細膩\n",
      "手機售價1萬6900元\n",
      "有灰\n",
      "銀兩色\n",
      "即日起於經銷通路開賣\n",
      "明日起在中華電信\n",
      "台灣大門市上市\n",
      " P9具備5.2吋FHD螢幕\n",
      "內建記憶體32 GB\n",
      "可擴充記憶卡\n",
      "前鏡頭800萬畫素\n",
      "主鏡頭採雙鏡頭設計\n",
      "兩個鏡頭皆為1200萬畫素\n",
      "一個為黑白一個為彩色鏡頭\n",
      "彩色鏡頭可還原色彩\n",
      "黑白系統則可捕捉細節\n",
      "即使在低光的條件下\n",
      "也能捕捉豐富的影像細節\n",
      " 使用者可選擇標準\n",
      "鮮艷\n",
      "柔和色彩等拍攝模式\n",
      "選擇單色模式\n",
      "P9就能化身黑白相機\n",
      "而利用混合對焦技術\n",
      "相機會透過雷射\n",
      "深度計算\n",
      "對比等3種方式精準對焦\n",
      "提升拍照精確度及穩定度\n",
      "華為表示\n",
      "P9鎖定年齡30至39歲的時尚商務族群\n",
      "而P9 Plus則預計在6月上市\n",
      "楊寧芷\n",
      "台北報導\n",
      " 出版\n",
      "14:14更新\n",
      "15:31 \n",
      "華為P9旗艦機發表會\n",
      "吳貞慧攝華為P9今上市\n",
      "楊寧芷攝\n"
     ]
    }
   ],
   "source": [
    "summary = df.ix[2]['summary'].encode('utf-8')\n",
    "#print summary\n",
    "import re\n",
    "for sentence in re.split('，|：|。|、|/|；|（|）', summary):\n",
    "    if sentence.strip() != '':\n",
    "        print sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary = df.ix[2]['summary'].encode('utf-8')\n",
    "c2 = Counter()\n",
    "import re\n",
    "for sentence in re.split('，|：|。|、|/|；|（|）', summary):\n",
    "    if sentence.strip() != '':\n",
    "        c = ngram(sentence.strip())\n",
    "        c2 = c2 + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P9 7\n",
      "鏡頭 7\n",
      "華為 4\n",
      "黑白 3\n",
      "00 3\n",
      "上市 3\n"
     ]
    }
   ],
   "source": [
    "for ele in  c2.most_common(30):\n",
    "    if ele[1] >= 3:\n",
    "        print ele[0], ele[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黑白系統則可捕捉細節\n",
      "黑白\n",
      "則可捕捉細節\n",
      "黑白則可捕捉細節\n",
      "黑白則可細節\n"
     ]
    }
   ],
   "source": [
    "a = '黑白系統則可捕捉細節'\n",
    "print a\n",
    "\n",
    "for ele in a.split('系統'):\n",
    "    print ele\n",
    "\n",
    "print ''.join(a.split('系統'))\n",
    "\n",
    "keywords = ['系統', '捕捉']\n",
    "for keyword in keywords:\n",
    "    a = ''.join(a.split(keyword))\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "則可細節\n"
     ]
    }
   ],
   "source": [
    "def removeKey(sentence, keywords):\n",
    "    for keyword in keywords:\n",
    "        sentence = ''.join(sentence.split(keyword))\n",
    "    return sentence    \n",
    "print removeKey('黑白系統則可捕捉細節',  ['系統', '捕捉', '黑白'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到底要不要\n"
     ]
    }
   ],
   "source": [
    "print removeKey('到底要不要大巨蛋',  ['大巨蛋'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a@b@c@d\n"
     ]
    }
   ],
   "source": [
    "a = [\"a\", \"b\", \"c\", \"d\"]\n",
    "print '@'.join(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實作長詞優先演算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-9d0e135cfc94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremoveKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#取ngram\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                 \u001b[0mcsummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsummary\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsummary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\collections.pyc\u001b[0m in \u001b[0;36m__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mnewcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnewcount\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m                 \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "summaries = df['summary']\n",
    "delimiter = '，|：|。|、|/|；|（|）|(|)|「|」|」|《|》|／|【|……'.decode('utf-8')\n",
    "keywords = []\n",
    "\n",
    "for i in range(4, 1, -1):\n",
    "    csummary = Counter()\n",
    "    for summary in summaries[0:100]:        \n",
    "        for sentence in re.split(delimiter, summary): # 斷句\n",
    "            if sentence is not None and sentence.strip() != '':\n",
    "                c = ngram(removeKey(sentence.encode('utf-8').strip(), keywords), i) #取ngram\n",
    "                csummary = csummary + c\n",
    "    print i\n",
    "    for ele in csummary:\n",
    "        if csummary[ele] >=30:\n",
    "            if re.match(u\"[\\u4e00-\\u9fa5]\", ele):\n",
    "                keywords.append(ele.encode('utf-8'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ele in keywords:\n",
    "    print ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summaries = df['summary']\n",
    "delimiter = '，|：|。|、|/|；|（|）|(|)|「|」|」|《|》|／|【|……'.decode('utf-8')\n",
    "keywords = []\n",
    "\n",
    "def getSentenceNgram(summary):\n",
    "    csummary = Counter()\n",
    "    for sentence in re.split(delimiter, summary): # 斷句\n",
    "        if sentence is not None and sentence.strip() != '':\n",
    "                c = ngram(removeKey(sentence.encode('utf-8').strip(), keywords), i) #取ngram\n",
    "                csummary = csummary + c\n",
    "    return csummary\n",
    "\n",
    "for i in range(4, 1, -1):\n",
    "    coverall = Counter()\n",
    "    for summary in summaries[0:100]:        \n",
    "        c = getSentenceNgram(summary) \n",
    "        coverall = coverall  + c\n",
    "    for ele in coverall:\n",
    "        if coverall[ele] >=40:\n",
    "            if re.match(u\"[\\u4e00-\\u9fa5]\", ele):\n",
    "                keywords.append(ele.encode('utf-8'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "台北報導\n",
      "中心\n",
      "警方\n",
      "提供\n",
      "台灣\n",
      "新聞\n",
      "北市\n",
      "更新\n",
      "報導\n",
      "美國\n",
      "時間\n",
      "表示\n",
      "翻攝\n",
      "智慧\n"
     ]
    }
   ],
   "source": [
    "for ele in keywords:\n",
    "    print ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "c['a'] += 1\n",
    "c['b'] += 100\n",
    "print c\n",
    "for ele in c:\n",
    "    print ele\n",
    "    \n",
    "c2 = Counter()\n",
    "c2['a'] += 2\n",
    "c2['b'] += 2\n",
    "\n",
    "print c2\n",
    "c+ c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba\n",
    "- https://pypi.python.org/packages/f6/86/9e721cc52075a07b7d07eb12bcb5dde771d35332a3dae1e14ae4290a197a/jieba-0.38.zip\n",
    "- pip install jieba-0.38.zip\n",
    "- C:\\Anaconda2\\Lib\\site-packages\\jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新：新增影片 華為今在台發表最新旗艦機P9，主打拍照功能，手機搭載兩顆徠卡（Leica）光學鏡頭，讓照片更細膩；手機售價1萬6900元，有灰、銀兩色，即日起於經銷通路開賣，明日起在中華電信、台灣大門市上市。 P9具備5.2吋FHD螢幕，內建記憶體32 GB，可擴充記憶卡，前鏡頭800萬畫素，主鏡頭採雙鏡頭設計，兩個鏡頭皆為1200萬畫素，一個為黑白一個為彩色鏡頭，彩色鏡頭可還原色彩，黑白系統則可捕捉細節，即使在低光的條件下，也能捕捉豐富的影像細節。 使用者可選擇標準、鮮艷、柔和色彩等拍攝模式，選擇單色模式，P9就能化身黑白相機，而利用混合對焦技術，相機會透過雷射、深度計算、對比等3種方式精準對焦，提升拍照精確度及穩定度；華為表示，P9鎖定年齡30至39歲的時尚商務族群，而P9 Plus則預計在6月上市。（楊寧芷/台北報導） 出版：14:14更新：15:31 \n",
      "華為P9旗艦機發表會。吳貞慧攝華為P9今上市。楊寧芷攝\n",
      "更新 ： 新增 影片  華為 今 在 台 發表 最新 旗艦機 P9 ， 主打 拍照 功能 ， 手機 搭載 兩顆 徠卡 （ Leica ） 光學鏡頭 ， 讓 照片 更細膩 ； 手機售價 1 萬 6900 元 ， 有 灰 、 銀兩色 ， 即日起 於 經銷 通路 開賣 ， 明日 起 在 中華電信 、 台灣 大門市 上市 。  P9 具備 5.2 吋 FHD 螢幕 ， 內建 記憶體 32   GB ， 可 擴充 記憶卡 ， 前鏡頭 800 萬畫素 ， 主鏡頭 採雙鏡頭 設計 ， 兩個 鏡頭 皆 為 1200 萬畫素 ， 一個 為 黑白 一個 為 彩色 鏡頭 ， 彩色 鏡頭 可 還原 色彩 ， 黑白 系統則 可 捕捉 細節 ， 即使 在 低光 的 條件 下 ， 也 能 捕捉 豐富 的 影像 細節 。  使用者 可選擇 標準 、 鮮艷 、 柔和 色彩 等 拍 攝 模式 ， 選擇 單色 模式 ， P9 就 能 化身 黑白 相機 ， 而 利用 混合 對 焦技術 ， 相機 會 透過 雷射 、 深度 計算 、 對比 等 3 種 方式 精準 對 焦 ， 提升 拍照 精確度 及 穩定度 ； 華為 表示 ， P9 鎖定 年齡 30 至 39 歲 的 時尚 商務 族群 ， 而 P9   Plus 則 預計 在 6 月 上市 。 （ 楊寧芷 / 台北 報導 ）  出版 ： 14 : 14 更新 ： 15 : 31  \n",
      "華為 P9 旗艦機 發表會 。 吳貞慧 攝華為 P9 今 上市 。 楊寧芷攝\n"
     ]
    }
   ],
   "source": [
    "summary = df.ix[2]['summary'].encode('utf-8')\n",
    "print summary\n",
    "\n",
    "jieba.add_word('徠卡')\n",
    "jieba.add_word('相機')\n",
    "jieba.add_word('對比')\n",
    "for w in jieba.cut(summary):\n",
    "    print w, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 大/ 巨蛋/ 案/ 對/ 市府/ 同仁/ 下/ 封口/ 封口令/ 口令/ / / 柯/ P/ 否/ 認\n",
      "Precise Mode: 大/ 巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/ 柯/ P/ 否認\n"
     ]
    }
   ],
   "source": [
    "seg_list=jieba.cut(\"大巨蛋案對市府同仁下封口令？柯P否認\",cut_all=True)\n",
    "print\"Full Mode:\",\"/ \".join(seg_list)\n",
    "\n",
    "\n",
    "seg_list=jieba.cut(\"大巨蛋案對市府同仁下封口令？柯P否認\",cut_all=False)\n",
    "print\"Precise Mode:\",\"/ \".join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precise Mode: 大/ 巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/ 柯P/ 否認\n"
     ]
    }
   ],
   "source": [
    "jieba.add_word('柯P')\n",
    "seg_list=jieba.cut(\"大巨蛋案對市府同仁下封口令？柯P否認\",cut_all=False)\n",
    "print\"Precise Mode:\",\"/ \".join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大 a\n",
      "巨蛋 n\n",
      "案 ng\n",
      "對 p\n",
      "市府 n\n",
      "同仁 nr\n",
      "下 f\n",
      "封口令 n\n",
      "？ x\n",
      "柯P n\n",
      "否認 v\n"
     ]
    }
   ],
   "source": [
    "jieba.add_word('柯P', 100, 'n')\n",
    "import jieba.posseg as pseg\n",
    "words=pseg.cut(\"大巨蛋案對市府同仁下封口令？柯P否認\")\n",
    "for w in words:\n",
    "    print w.word,w.flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大 0 1\n",
      "巨蛋 1 3\n",
      "案對 3 5\n",
      "市府 5 7\n",
      "同仁 7 9\n",
      "下 9 10\n",
      "封口令 10 13\n",
      "？ 13 14\n",
      "柯P 14 16\n",
      "否認 16 18\n"
     ]
    }
   ],
   "source": [
    "sentence = '大巨蛋案對市府同仁下封口令？柯P否認'\n",
    "words=jieba.tokenize(unicode(sentence,'utf-8'))\n",
    "for tw in words:\n",
    "    print tw[0],tw[1],tw[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "巨蛋\n",
      "同仁\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse \n",
    "tags=jieba.analyse.extract_tags(sentence,1)\n",
    "print\",\".join(tags)\n",
    "\n",
    "tags=jieba.analyse.extract_tags(sentence,1,allowPOS=['nr'])\n",
    "print\",\".join(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "1 3\n",
      "2 4\n",
      "3 5\n",
      "4 6\n"
     ]
    }
   ],
   "source": [
    "a = [2,3,4,5,6]\n",
    "cnt = 0 \n",
    "for ele in a:\n",
    "    print cnt, ele\n",
    "    cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "1 3\n",
      "2 4\n",
      "3 5\n",
      "4 6\n"
     ]
    }
   ],
   "source": [
    "for idx, ele in enumerate(a):\n",
    "    print idx, ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "c = Counter()\n",
    "summaries = df['summary']\n",
    "for idx, summary in enumerate(summaries):\n",
    "    for ele in jieba.cut(summary.encode('utf-8')):\n",
    "        c[ele] +=1\n",
    "    #print idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "for ele in c.most_common(300):\n",
    "    if len(ele[0]) >= 2 and re.match(u\"[\\u4e00-\\u9fa5]\", ele[0]):\n",
    "        #print ele[0], ele[1]\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a, b = 2,3\n",
    "print a \n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 0.0, 0.0)\n",
      "(0.3333333333333333, 0.0, 0.0)\n",
      "(0.3333333333333333, 0.0, 0.0)\n",
      "(0.6666666666666666, 0.40546510810816438, 0.27031007207210955)\n",
      "(0.3333333333333333, 0.40546510810816438, 0.13515503603605478)\n",
      "(0.3333333333333333, 1.0986122886681098, 0.36620409622270322)\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "def tfidf(t,d,D):\n",
    "    tf=float(d.count(t))/sum(d.count(w) for w in set(d))\n",
    "    idf=sp.log(float(len(D))/(len([doc for doc in D if t in doc])))\n",
    "    return tf, idf, tf*idf\n",
    "\n",
    "a,abb,abc=[\"a\"],[\"a\",\"b\",\"b\"],[\"a\",\"b\",\"c\"]\n",
    "\n",
    "D=[a,abb,abc]\n",
    "print tfidf('a', a, D)\n",
    "print tfidf('a', abb, D)\n",
    "print tfidf('a', abc, D)\n",
    "\n",
    "print tfidf('b', abb, D)\n",
    "print tfidf('b', abc, D)\n",
    "\n",
    "print tfidf('c', abc, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天 天氣 真好 今天 晚上 會 下雨\n"
     ]
    }
   ],
   "source": [
    "a =  '今天天氣真好'\n",
    "b =  '今天晚上會下雨'\n",
    "for ele in jieba.cut(a):\n",
    "    print ele, \n",
    "    \n",
    "for ele in jieba.cut(b):\n",
    "    print ele, "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    今天 天氣 真好 晚上 會 下雨\n",
    "a    1    1    1    0  0   0\n",
    "b    1    0    0    1  1   1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\u4eca\\u5929 \\u5929\\u6c23 \\u771f\\u597d', u'\\u4eca\\u5929 \\u665a\\u4e0a \\u6703 \\u4e0b\\u96e8']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "corpus.append(' '.join(jieba.cut(a)))\n",
    "corpus.append(' '.join(jieba.cut(b)))\n",
    "print corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下雨 今天 天氣 晚上 真好\n",
      "[[0 1 1 0 1]\n",
      " [1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "\n",
    "word=vectorizer.get_feature_names()\n",
    "for w in word:\n",
    "    print w, \n",
    "print \n",
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba \n",
    "ary=['【更新】柯P：洪智坤洩漏公文案還沒看到公文今處理',\n",
    "     '留洪智坤柯：殘障求職不易',\n",
    "     '人事處議處洪智坤柯P：不清楚議處結果']\n",
    "corpus=[]\n",
    "for title in ary:\n",
    "    corpus.append(' '.join(jieba.cut(title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不易 人事 今處理 公文 更新 柯p 案還 殘障 求職 洩漏 洪智坤 清楚 留洪智坤柯 看到 結果 處議 議處\n",
      "[[0 0 1 2 1 1 1 0 0 1 1 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "\n",
    "word=vectorizer.get_feature_names()\n",
    "for w in word:\n",
    "    print w, \n",
    "print \n",
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba.add_word('洪智坤')\n",
    "import jieba \n",
    "ary=['【更新】柯P：洪智坤洩漏公文案還沒看到公文今處理',\n",
    "     '留洪智坤柯：殘障求職不易',\n",
    "     '人事處議處洪智坤柯P：不清楚議處結果']\n",
    "corpus=[]\n",
    "for title in ary:\n",
    "    corpus.append(' '.join(jieba.cut(title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不易 人事 今處理 公文 更新 柯p 案還 殘障 求職 洩漏 洪智坤 清楚 看到 結果 處議 議處\n",
      "[[0 0 1 2 1 1 1 0 0 1 1 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "\n",
    "word=vectorizer.get_feature_names()\n",
    "for w in word:\n",
    "    print w, \n",
    "print \n",
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不易 人事 今處理 公文 更新 柯p 案還 殘障 求職 洩漏 洪智坤 清楚 看到 結果 處議 議處\n",
      "[[ 0.          0.          0.31738473  0.63476946  0.31738473  0.24137927\n",
      "   0.31738473  0.          0.          0.31738473  0.18745253  0.\n",
      "   0.31738473  0.          0.          0.        ]\n",
      " [ 0.54645401  0.          0.          0.          0.          0.          0.\n",
      "   0.54645401  0.54645401  0.          0.32274454  0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.41074684  0.          0.          0.          0.31238356\n",
      "   0.          0.          0.          0.          0.2425937   0.41074684\n",
      "   0.          0.41074684  0.41074684  0.41074684]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer=TfidfTransformer()\n",
    "tfidf=transformer.fit_transform(X)\n",
    "weight=tfidf.toarray()\n",
    "word=vectorizer.get_feature_names()\n",
    "for w in word:\n",
    "    print w, \n",
    "print \n",
    "print weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.06049928  0.12087772]\n",
      " [ 0.06049928  1.          0.07829579]\n",
      " [ 0.12087772  0.07829579  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "print linear_kernel(tfidf,tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.06049928  0.12087772]]\n",
      "[ 1.          0.06049928  0.12087772]\n",
      "[ 1.          0.06049928  0.12087772]\n"
     ]
    }
   ],
   "source": [
    "print linear_kernel(tfidf[0],tfidf)\n",
    "print linear_kernel(tfidf[0],tfidf).flatten()\n",
    "cosine_similarities=linear_kernel(tfidf[0],tfidf).flatten()\n",
    "print cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人事處議處洪智坤柯P：不清楚議處結果 0.120877716968\n",
      "留洪智坤柯：殘障求職不易 0.0604992822195\n"
     ]
    }
   ],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "for doc in related_docs_indices[1:]:\n",
    "    print ary[doc], cosine_similarities[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "import pandas as pd\n",
    "with lite.connect('news.sqlite') as db:\n",
    "    cur = db.cursor()\n",
    "    cur.execute('select * from news_entry;')\n",
    "    data = cur.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba \n",
    "\n",
    "corpus = []\n",
    "titles = []\n",
    "for rec in data:\n",
    "    corpus.append(' '.join(jieba.cut(rec[5])))\n",
    "    titles.append(rec[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer=TfidfTransformer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "tfidf=transformer.fit_transform(X)\n",
    "weight=tfidf.toarray()\n",
    "word=vectorizer.get_feature_names()\n",
    "\n",
    "#for w in word:\n",
    "#    print w, \n",
    "#print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【民報書摘】地圖上不存在的國家\n",
      "【更新】殺1、2人不會判死？   割喉殺...\n",
      "華為徠卡旗艦機　1萬7千元有找\n",
      "哈孝遠相揪尬籃球　一句話竟然讓JJ硬了！\n",
      "【央廣RTI】遊古巴正夯  美國人暴增9...\n",
      "中國守株待兔，卻將物極必反\n",
      "湯姆熊殺童魔無期定讞　被害童姑姑「無法接...\n",
      "林佳龍六都滿意度亞軍　議員拉布條同賀\n",
      "曾自爆和死亡擦身　李敖現身北檢裝神祕\n",
      "海上漂流近月　4男被台灣漁船救起\n",
      "拖一年半未解　議員：大巨蛋成柯P政治墳場\n",
      "【更新】過去批「黑箱協商」　時力首度召集...\n",
      "我們的風光是用命換來的\n",
      "轉運站彩券行刮出千萬頭獎　台彩今致賀\n",
      "誆內急闖屋性騷女房客　色房東竟翻聞內褲\n",
      "外勞坐滿一大台遊覽車　全帶移民署清查\n",
      "高雄輕軌C4~C8將通車　周邊房市詢問升...\n",
      "電子菸是「戒」菸？還是「借」菸？\n",
      "妙齡女控遭迷姦　8點脫身11點赴約3P\n",
      "陸監管網路直播　禁挑逗性吃香蕉\n"
     ]
    }
   ],
   "source": [
    "for title in titles[0:20]:\n",
    "    print title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "華為徠卡旗艦機　1萬7千元有找\n"
     ]
    }
   ],
   "source": [
    "print titles[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.94323846e-03   1.52521081e-02   1.00000000e+00 ...,   4.63720330e-04\n",
      "   6.60805265e-03   5.11144864e-03]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tfidf[2],tfidf).flatten()\n",
    "print cosine_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【更新】首款徠卡認證雙鏡機皇　價格有驚喜 0.3961681206\n",
      "華為雙鏡頭P9手機　1.69萬在台上市 0.312849995273\n",
      "新鏡頭尬大光圈　外型根本山寨 0.134797765423\n"
     ]
    }
   ],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "for doc in related_docs_indices[1:]:\n",
    "    if cosine_similarities[doc] >= 0.1:\n",
    "        print titles[doc], cosine_similarities[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "def getSimiliarArticle(idx):\n",
    "    print \"[查詢文章]:\" , titles[idx]\n",
    "    cosine_similarities = linear_kernel(tfidf[idx],tfidf).flatten()\n",
    "    related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "    for doc in related_docs_indices[1:]:\n",
    "        if cosine_similarities[doc] >= 0.1:\n",
    "            print \"[相似文章]:\", titles[doc], cosine_similarities[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[查詢文章]: 【央廣RTI】憂茲卡病毒  高球名將相繼...\n",
      "[相似文章]: 恭喜台灣桌球！　奧運男、女6席滿額 0.197256924597\n",
      "[相似文章]: 【央廣RTI】蠻牛納達爾掌旗  西班牙進... 0.190598113017\n",
      "[相似文章]: 日羽球名將奧運涉賭遭除名   取代的是.... 0.150959587435\n",
      "[相似文章]: 【法廣RFI】政治危機陰影中 奧運聖火開... 0.146220345922\n",
      "[相似文章]: 【更新】挑戰奧運資格竟敗給豔陽 0.135686426965\n",
      "[相似文章]: 【圖解新聞】酒精殺不死的腸病毒　怎麼預防 0.115406559905\n"
     ]
    }
   ],
   "source": [
    "getSimiliarArticle(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering news with kemans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba \n",
    "\n",
    "corpus = []\n",
    "titles = []\n",
    "for rec in data[0:300]:\n",
    "    corpus.append(' '.join(jieba.cut(rec[5])))\n",
    "    titles.append(rec[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "weight = tfidf.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "c = cluster.KMeans(n_clusters=6)\n",
    "k_data = c.fit_predict(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "林佳龍六都滿意度亞軍　議員拉布條同賀\n",
      "【影片】警局前大迴轉　害後方騎士撞上重傷\n",
      "【影片】騎士單場飆25記3分球　打破NB...\n",
      "【影片】王建民今日表現　2局影片看這邊\n",
      "F奶國民偶像下海熱賣　橫掃AV奧斯卡4獎\n",
      "【央廣RTI】全場25個3分球  騎士創...\n",
      "婦人騎車摔傷　運將撐傘遮烈陽降溫\n",
      "飛機撞老鷹致空難4死　全美首例\n",
      "胡智為6局飆8K　黃暐傑幸運勝投\n",
      "倫敦市長今選舉　可能出現首位穆斯林市長 \n",
      " 【影片】騎士輕取老鷹　季後賽對戰10戰...\n",
      "倫敦市長今選舉　可能出現首位穆斯林市長 \n",
      " 【影片】騎士輕取老鷹　季後賽對戰10戰...\n",
      "涉賭裁判爆料　100%的NBA裁判有在賭...\n",
      "逆向女騎士也有優先路權？　鄉民熱議\n",
      "【影片】勇士格林「當機」　當場癡呆20秒\n",
      "【央廣RTI】騎士半場18發3分彈  創...\n",
      "生涯第2度「連2日出賽」　王建民表現稱職\n",
      "好友變敵人？詹姆士約戰韋德東區決賽\n",
      "【更新】《望春風》好催淚！85歲奶奶拄著...\n"
     ]
    }
   ],
   "source": [
    "for idx, group in enumerate(k_data):\n",
    "    if group == 5:\n",
    "        print titles[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusteirng 6/28 news with kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "from xml.etree import ElementTree\n",
    "import jieba.analyse\n",
    "\n",
    "f = open('1435449602.xml', 'r')\n",
    "events=ElementTree.fromstring(f.read())\n",
    "f.close()\n",
    "\n",
    "corpus = []\n",
    "ary    = []\n",
    "src    = []\n",
    "for elem in events.findall('./channel/item'):\n",
    "    guid        = elem.find('guid').text\n",
    "    title       = elem.find('title').text\n",
    "    description = elem.find('description').text\n",
    "    pubDate     = elem.find('pubDate').text\n",
    "    source      = elem.find('source').text\n",
    "    src.append(source)\n",
    "    ary.append(title)\n",
    "    corpus.append(' '.join(jieba.analyse.extract_tags(description, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "weight = tfidf.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "c = cluster.KMeans(n_clusters=6)\n",
    "k_data = c.fit_predict(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "八仙塵爆  五相關人依公共危險重傷害法辦\n",
      "6月28日各報頭版要聞\n",
      "八仙樂園舞台大火 逾400人輕重傷\n",
      "八仙樂園貼出暫停營業海報\n",
      "八仙樂園爆炸案 專家認玉米粉危險性高應管制使用\n",
      "八仙樂園粉塵燃爆 400人傷 負責人被移送\n",
      "蘇塞大屠殺 凶手畫面曝光\n",
      "新生代性感女神降臨　男粉絲擠爆簽書會\n",
      "侯友宜探視傷者：追究責任\n",
      "八仙樂園火警  國軍加入救援\n",
      "八仙樂園爆炸案災害應變中心　1時30分一級開設\n",
      "八仙派對彩粉釀大禍  衛福部：研議加強管理\n",
      "彩色派對主辦人：風勢太大，引燃粉塵\n",
      "八仙樂園大火  傷者查詢電話\n",
      "派對彩粉首次釀禍  蔣丙煌：研議管理\n",
      "八仙樂園火警受傷名單一覽表\n",
      "00：35統計　八仙大火重傷97人、輕傷132人\n",
      "衛福部長：北北基醫護人員全力動員\n",
      "八仙樂園粉塵瞬燃　還原失事現場影片曝光\n",
      "八仙樂園爆炸案 士檢派3檢察官現場指揮\n",
      "八仙樂園意外 洪秀柱：為傷者祈福\n",
      "八仙樂園爆炸228人輕重傷 朱立倫：無限期停業\n",
      "八仙樂園爆炸意外 朱立倫：立即停園接受調查\n",
      "八仙樂園塵爆215傷　朱立倫下令即刻停業\n",
      "八仙樂園爆炸 朱立倫4點指示\n",
      "【八仙意外】馬英九、毛治國第一時間得知　指示全\n",
      "Gap徵小小代言人 艾力克斯親子秀時尚\n",
      "【八仙意外】讓專業的來！柯文哲指示啟動EOC\n",
      "八仙水上樂園爆炸逾百人傷　三軍總醫院收治傷患\n"
     ]
    }
   ],
   "source": [
    "for idx, group in enumerate(k_data):\n",
    "    if group == 3:\n",
    "        print ary[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
